{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention-ConvLSTM",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OimtV0Cch-6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader, random_split\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet34\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import wandb\n",
        "from datetime import datetime\n",
        "from utils import Config, test\n",
        "from gtea_dataset import gtea61"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu8fi6SJiGre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wandb login ***"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnaCpjbbiIpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config_stage1 = Config({\"stage\": 1,\n",
        "                        \"num_classes\": 61,\n",
        "                        \"batch_size\": 32,\n",
        "                        \"lstm_mem_size\": 512,\n",
        "                        \"lr\": 1e-3,\n",
        "                        \"optimizer\": \"adam\",\n",
        "                        \"epochs\": 10,  # 200 TODO\n",
        "                        \"decay_steps\": [25, 75, 150],\n",
        "                        \"decay_factor\": 0.1,\n",
        "                        \"weight_decay\": 5e-5,\n",
        "                        \"val_frequency\": 3,\n",
        "                        \"models_dir\": \"models\",\n",
        "                        \"seq_len\": 7,\n",
        "                        \"training_user_split\": [1, 3, 4],\n",
        "                        \"val_user_split\": [2]})\n",
        "\n",
        "config_stage2 = Config({\"stage\": 2,\n",
        "                        \"num_classes\": 61,\n",
        "                        \"batch_size\": 32,\n",
        "                        \"lstm_mem_size\": 512,\n",
        "                        \"lr\": 1e-4,\n",
        "                        \"optimizer\": \"adam\",\n",
        "                        \"epochs\": 10, # 150 TODO\n",
        "                        \"decay_steps\": [25, 75],\n",
        "                        \"decay_factor\": 0.1,\n",
        "                        \"weight_decay\": 5e-5,\n",
        "                        \"val_frequency\": 3,\n",
        "                        \"models_dir\": \"models\",\n",
        "                        \"seq_len\": 7,\n",
        "                        \"training_user_split\": [1, 3, 4],\n",
        "                        \"val_user_split\": [2]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaJapH1MifSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.isdir('./GTEA61'):\n",
        "  !git clone https://github.com/MauriVass/GTEA61\n",
        "\n",
        "if not os.path.isdir('./'+config_stage1.mdoels_dir):\n",
        "  os.mkdir('./'+config_stage1.mdoels_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jojb6AcYiLme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PrepareTraining(config):\n",
        "    train_params = []\n",
        "    if config.stage == 1:\n",
        "\n",
        "        model = attentionModel(num_classes=config.num_classes, mem_size=config.lstm_mem_size)\n",
        "        model.train(False)\n",
        "        for params in model.parameters():\n",
        "            params.requires_grad = False\n",
        "\n",
        "    else:\n",
        "\n",
        "        model = attentionModel(num_classes=config.num_classes, mem_size=config.lstm_mem_size)\n",
        "        stage1_dict = config.models_dir + '/best_model_rgb_state_dict.pth'\n",
        "        model.load_state_dict(torch.load(stage1_dict))\n",
        "        model.train(False)\n",
        "        for params in model.parameters():\n",
        "            params.requires_grad = False\n",
        "        #\n",
        "        for params in model.resNet.layer4[0].conv1.parameters():\n",
        "            params.requires_grad = True\n",
        "            train_params += [params]\n",
        "\n",
        "        for params in model.resNet.layer4[0].conv2.parameters():\n",
        "            params.requires_grad = True\n",
        "            train_params += [params]\n",
        "\n",
        "        for params in model.resNet.layer4[1].conv1.parameters():\n",
        "            params.requires_grad = True\n",
        "            train_params += [params]\n",
        "\n",
        "        for params in model.resNet.layer4[1].conv2.parameters():\n",
        "            params.requires_grad = True\n",
        "            train_params += [params]\n",
        "\n",
        "        for params in model.resNet.layer4[2].conv1.parameters():\n",
        "            params.requires_grad = True\n",
        "            train_params += [params]\n",
        "        #\n",
        "        for params in model.resNet.layer4[2].conv2.parameters():\n",
        "            params.requires_grad = True\n",
        "            train_params += [params]\n",
        "        #\n",
        "        for params in model.resNet.fc.parameters():\n",
        "            params.requires_grad = True\n",
        "            train_params += [params]\n",
        "\n",
        "        model.resNet.layer4[0].conv1.train(True)\n",
        "        model.resNet.layer4[0].conv2.train(True)\n",
        "        model.resNet.layer4[1].conv1.train(True)\n",
        "        model.resNet.layer4[1].conv2.train(True)\n",
        "        model.resNet.layer4[2].conv1.train(True)\n",
        "        model.resNet.layer4[2].conv2.train(True)\n",
        "        model.resNet.fc.train(True)\n",
        "\n",
        "    for params in model.lstm_cell.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "\n",
        "    for params in model.classifier.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "\n",
        "    return model, train_params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3paF7csiNRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TrainingRGB(model, config):\n",
        "    wandb.watch(model, log=\"all\")\n",
        "    train_iter = 0\n",
        "    best_accuracy = 0\n",
        "    train = []\n",
        "    val = []\n",
        "    for epoch in range(config.epochs):\n",
        "        epoch_loss = 0\n",
        "        numCorrTrain = 0\n",
        "        trainSamples = 0\n",
        "        iterPerEpoch = 0\n",
        "        model.lstm_cell.train(True)\n",
        "        model.classifier.train(True)\n",
        "        # writer.add_scalar('lr', optimizer_fn.param_groups[0]['lr'], epoch+1)\n",
        "        if config.stage == 2:\n",
        "            model.resNet.layer4[0].conv1.train(True)\n",
        "            model.resNet.layer4[0].conv2.train(True)\n",
        "            model.resNet.layer4[1].conv1.train(True)\n",
        "            model.resNet.layer4[1].conv2.train(True)\n",
        "            model.resNet.layer4[2].conv1.train(True)\n",
        "            model.resNet.layer4[2].conv2.train(True)\n",
        "            model.resNet.fc.train(True)\n",
        "        for inputs, labels in train_loader:\n",
        "            train_iter += 1\n",
        "            iterPerEpoch += 1\n",
        "            optimizer_fn.zero_grad()\n",
        "            trainSamples += inputs.size(0)\n",
        "            inputs = inputs.permute(1, 0, 2, 3, 4).to(config.device)  # but why?\n",
        "            labels = labels.to(config.device)\n",
        "            output_label, _ = model(inputs)\n",
        "            loss = loss_fn(output_label, labels)\n",
        "            loss.backward()\n",
        "            optimizer_fn.step()\n",
        "            _, predicted = torch.max(output_label.data, 1)\n",
        "\n",
        "            predicted = predicted.to(config.device)\n",
        "            numCorrTrain += torch.sum(predicted == labels).data.item()\n",
        "\n",
        "            # numCorrTrain += (predicted == targets.cuda()).sum()\n",
        "            epoch_loss += loss.item()\n",
        "        optim_scheduler.step()\n",
        "        avg_loss = epoch_loss / iterPerEpoch\n",
        "        trainAccuracy = (numCorrTrain / trainSamples)\n",
        "\n",
        "        print('Train: Epoch = {}/{} | Loss = {} | Accuracy = {}'.format(epoch + 1, config.epochs, avg_loss, trainAccuracy))\n",
        "\n",
        "        max_loss = 6\n",
        "        avg_loss_normalized = avg_loss if avg_loss < max_loss else max_loss\n",
        "        train.append((trainAccuracy, avg_loss_normalized))\n",
        "        wandb.log({\"train_loss\": avg_loss_normalized,\n",
        "                   \"train_accuracy\": trainAccuracy,\n",
        "                   \"eopch\": (epoch + 1)})\n",
        "\n",
        "        if (epoch + 1) % config.val_frequency == 0:\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                val_loss_epoch = 0\n",
        "                val_iter = 0\n",
        "                val_samples = 0\n",
        "                numCorr = 0\n",
        "                for inputs, labels in val_loader:\n",
        "                    val_iter += 1\n",
        "                    val_samples += inputs.size(0)\n",
        "                    inputs = inputs.permute(1, 0, 2, 3, 4).to(config.device)\n",
        "                    labels = labels.to(config.device)\n",
        "                    output_label, _ = model(inputs)\n",
        "                    val_loss = loss_fn(output_label, labels)\n",
        "                    val_loss_epoch += val_loss.item()\n",
        "                    _, predicted = torch.max(output_label.data, 1)\n",
        "                    numCorr += torch.sum(predicted == labels).data.item()\n",
        "            val_accuracy = (numCorr / val_samples)\n",
        "            avg_val_loss = val_loss_epoch / val_iter\n",
        "            print('*****  Val: Epoch = {} | Loss {} | Accuracy = {} *****'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
        "\n",
        "            avg_val_loss_normalized = avg_val_loss if avg_val_loss < max_loss else max_loss\n",
        "            val.append((val_accuracy, avg_val_loss_normalized))\n",
        "            wandb.log({\"valid_loss\": avg_val_loss_normalized,\n",
        "                       \"valid_accuracy\": val_accuracy,\n",
        "                       \"eopch\": (epoch + 1)})\n",
        "\n",
        "            if val_accuracy > best_accuracy:\n",
        "                save_path_model = (config.models_dir + '/best_model_rgb_state_dict.pth')\n",
        "                torch.save(model.state_dict(), save_path_model)\n",
        "                best_accuracy = val_accuracy\n",
        "        else:\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                save_path_model = (config.models_dir + '/model_rgb_state_dict_epoch' + str(epoch + 1) + '.pth')\n",
        "                # torch.save(model.state_dict(), save_path_model)\n",
        "    wandb.run.summary[\"best_valid_accuracy\"] = best_accuracy\n",
        "    return train, val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3PWFNY_iTug",
        "colab_type": "text"
      },
      "source": [
        "Prepare Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiY0dGkmiS4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gtea_dataset import gtea61\n",
        "from spatial_transforms import *\n",
        "from objectAttentionModelConvLSTM import *\n",
        "\n",
        "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "spatial_transform = Compose([Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n",
        "                             ToTensor(), normalize])\n",
        "\n",
        "gtea_root = \"GTEA61\"\n",
        "training_user_split = [1, 3, 4]\n",
        "val_user_split = [2]\n",
        "config = config_stage1\n",
        "train_dataset = gtea61(\"rgb\", gtea_root, split=\"train\", user_split=training_user_split, seq_len_rgb=config.seq_len, transform_rgb=spatial_transform, preload=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
        "\n",
        "val_transform = Compose([Scale(256), CenterCrop(224), ToTensor(), normalize])\n",
        "val_dataset = gtea61(\"rgb\", gtea_root, split=\"test\", user_split=val_user_split, seq_len_rgb=config.seq_len, transform_rgb=val_transform, preload=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=config.batch_size, shuffle=True, num_workers=0, pin_memory=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9-ANuGoiXki",
        "colab_type": "text"
      },
      "source": [
        "Train Stage 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6NmSOuKiW_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = config_stage1\n",
        "\n",
        "model, train_params = PrepareTraining(config)\n",
        "model.to(config.device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer_fn = torch.optim.Adam(train_params, lr=config.lr, weight_decay=config.weight_decay, eps=1e-4)\n",
        "optim_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=config.decay_steps, gamma=config.decay_factor)\n",
        "\n",
        "training_time = datetime.now().strftime(\"%d-%b_%H-%M\")\n",
        "wandb.init(config=config, group=f\"{config.seq_len}f\", name=f\"{training_time} Stage1, {config.seq_len}f, T{str(config.training_user_split).replace(' ', '')}\", project=\"mldl-fpar\")\n",
        "\n",
        "train_rgb, val_rgb = TrainingRGB(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhlMePN2iaq7",
        "colab_type": "text"
      },
      "source": [
        "Train Stage 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVxgDka8iZLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = config_stage2\n",
        "\n",
        "model, train_params2 = PrepareTraining(stage)\n",
        "model.lstm_cell.train(True)\n",
        "model.classifier.train(True)\n",
        "model.to(config.device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer_fn = torch.optim.Adam(train_params2, lr=config.lr, weight_decay=config.weight_decay, eps=1e-4)\n",
        "optim_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=config.decay_steps, gamma=config.decay_factor)\n",
        "\n",
        "\n",
        "training_time = datetime.now().strftime(\"%d-%b_%H-%M\")\n",
        "wandb.init(config=config, group=f\"{config.seq_len}f\", name=f\"{training_time} Stage2, {config.seq_len}f, T{str(config.training_user_split).replace(' ', '')}\", project=\"mldl-fpar\")\n",
        "\n",
        "train_rgb, val_rgb = TrainingRGB(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}