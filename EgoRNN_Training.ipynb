{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EgoRNN_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mldl2020/FirstPersonActionRecognition/blob/master/EgoRNN_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViVAkUndllg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install 'torch==1.4.0'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\n",
        "!pip3 install 'wandb'\n",
        "import os \n",
        "os._exit(00)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOgRVqUFk3kw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OimtV0Cch-6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader, random_split\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet34\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import wandb\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaJapH1MifSU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "94400d81-ebf6-40bf-bd0d-a7aecdaa8187"
      },
      "source": [
        "import os \n",
        "if not os.path.isdir('./FirstPersonActionRecognition'):\n",
        "    !git clone https://github.com/mldl2020/FirstPersonActionRecognition.git\n",
        "    !cp ./FirstPersonActionRecognition/*.py ./\n",
        "\n",
        "if not os.path.isdir('./GTEA61'):\n",
        "    !git clone https://github.com/MauriVass/GTEA61\n",
        "\n",
        "if not os.path.isdir(\"models\"):\n",
        "    os.mkdir(\"models\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'FirstPersonActionRecognition'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/46)\u001b[K\rremote: Counting objects:   4% (2/46)\u001b[K\rremote: Counting objects:   6% (3/46)\u001b[K\rremote: Counting objects:   8% (4/46)\u001b[K\rremote: Counting objects:  10% (5/46)\u001b[K\rremote: Counting objects:  13% (6/46)\u001b[K\rremote: Counting objects:  15% (7/46)\u001b[K\rremote: Counting objects:  17% (8/46)\u001b[K\rremote: Counting objects:  19% (9/46)\u001b[K\rremote: Counting objects:  21% (10/46)\u001b[K\rremote: Counting objects:  23% (11/46)\u001b[K\rremote: Counting objects:  26% (12/46)\u001b[K\rremote: Counting objects:  28% (13/46)\u001b[K\rremote: Counting objects:  30% (14/46)\u001b[K\rremote: Counting objects:  32% (15/46)\u001b[K\rremote: Counting objects:  34% (16/46)\u001b[K\rremote: Counting objects:  36% (17/46)\u001b[K\rremote: Counting objects:  39% (18/46)\u001b[K\rremote: Counting objects:  41% (19/46)\u001b[K\rremote: Counting objects:  43% (20/46)\u001b[K\rremote: Counting objects:  45% (21/46)\u001b[K\rremote: Counting objects:  47% (22/46)\u001b[K\rremote: Counting objects:  50% (23/46)\u001b[K\rremote: Counting objects:  52% (24/46)\u001b[K\rremote: Counting objects:  54% (25/46)\u001b[K\rremote: Counting objects:  56% (26/46)\u001b[K\rremote: Counting objects:  58% (27/46)\u001b[K\rremote: Counting objects:  60% (28/46)\u001b[K\rremote: Counting objects:  63% (29/46)\u001b[K\rremote: Counting objects:  65% (30/46)\u001b[K\rremote: Counting objects:  67% (31/46)\u001b[K\rremote: Counting objects:  69% (32/46)\u001b[K\rremote: Counting objects:  71% (33/46)\u001b[K\rremote: Counting objects:  73% (34/46)\u001b[K\rremote: Counting objects:  76% (35/46)\u001b[K\rremote: Counting objects:  78% (36/46)\u001b[K\rremote: Counting objects:  80% (37/46)\u001b[K\rremote: Counting objects:  82% (38/46)\u001b[K\rremote: Counting objects:  84% (39/46)\u001b[K\rremote: Counting objects:  86% (40/46)\u001b[K\rremote: Counting objects:  89% (41/46)\u001b[K\rremote: Counting objects:  91% (42/46)\u001b[K\rremote: Counting objects:  93% (43/46)\u001b[K\rremote: Counting objects:  95% (44/46)\u001b[K\rremote: Counting objects:  97% (45/46)\u001b[K\rremote: Counting objects: 100% (46/46)\u001b[K\rremote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 46 (delta 23), reused 24 (delta 9), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (46/46), done.\n",
            "Cloning into 'GTEA61'...\n",
            "remote: Enumerating objects: 9639, done.\u001b[K\n",
            "remote: Counting objects: 100% (9639/9639), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9566/9566), done.\u001b[K\n",
            "remote: Total 93346 (delta 25), reused 9577 (delta 8), pack-reused 83707\u001b[K\n",
            "Receiving objects: 100% (93346/93346), 4.21 GiB | 23.30 MiB/s, done.\n",
            "Resolving deltas: 100% (1493/1493), done.\n",
            "Checking out files: 100% (90815/90815), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu8fi6SJiGre",
        "colab_type": "code",
        "outputId": "842b2b04-b842-4e13-f5ad-706220e38c5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# !wandb login ***"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRELuRUdiD20",
        "colab_type": "text"
      },
      "source": [
        "# RGB Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnaCpjbbiIpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from utils import Config\n",
        "\n",
        "config_stage1 = Config({\"stage\": 1,\n",
        "                        \"num_classes\": 61,\n",
        "                        \"batch_size\": 32,\n",
        "                        \"lstm_mem_size\": 512,\n",
        "                        \"lr\": 1e-3,\n",
        "                        \"optimizer\": \"adam\",\n",
        "                        \"epochs\": 200,\n",
        "                        \"decay_steps\": [25, 75, 150],\n",
        "                        \"decay_factor\": 0.1,\n",
        "                        \"weight_decay\": 5e-5,\n",
        "                        \"val_frequency\": 3,\n",
        "                        \"models_dir\": \"models\",\n",
        "                        \"seq_len\": 7,\n",
        "                        \"training_user_split\": [1, 3, 4],\n",
        "                        \"val_user_split\": [2]})\n",
        "\n",
        "config_stage2 = Config({\"stage\": 2,\n",
        "                        \"num_classes\": 61,\n",
        "                        \"batch_size\": 32,\n",
        "                        \"lstm_mem_size\": 512,\n",
        "                        \"lr\": 1e-4,\n",
        "                        \"optimizer\": \"adam\",\n",
        "                        \"epochs\": 150,\n",
        "                        \"decay_steps\": [25, 75],\n",
        "                        \"decay_factor\": 0.1,\n",
        "                        \"weight_decay\": 5e-5,\n",
        "                        \"val_frequency\": 3,\n",
        "                        \"models_dir\": \"models\",\n",
        "                        \"seq_len\": 7,\n",
        "                        \"training_user_split\": [1, 3, 4],\n",
        "                        \"val_user_split\": [2]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jojb6AcYiLme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_training_rgb(config):\n",
        "    train_params = []\n",
        "    if config.stage == 1:\n",
        "\n",
        "        model = attentionModel(num_classes=config.num_classes, mem_size=config.lstm_mem_size)\n",
        "        model.train(False)\n",
        "        for params in model.parameters():\n",
        "            params.requires_grad = False\n",
        "\n",
        "    else:\n",
        "\n",
        "        model = attentionModel(num_classes=config.num_classes, mem_size=config.lstm_mem_size)\n",
        "        stage1_dict = config.models_dir + '/best_model_rgb_state_dict.pth'\n",
        "        model.load_state_dict(torch.load(stage1_dict))\n",
        "        model.train(False)\n",
        "        for params in model.parameters():\n",
        "            params.requires_grad = False\n",
        "        #\n",
        "        for params in model.resNet.layer4[0].conv1.parameters():\n",
        "            params.requires_grad = True\n",
        "            train_params += [params]\n",
        "\n",
        "        for params in model.resNet.layer4[0].conv2.parameters():\n",
        "            params.requires_grad = True\n",
        "            train_params += [params]\n",
        "\n",
        "        for params in model.resNet.layer4[1].conv1.parameters():\n",
        "            params.requires_grad = True\n",
        "            train_params += [params]\n",
        "\n",
        "        for params in model.resNet.layer4[1].conv2.parameters():\n",
        "            params.requires_grad = True\n",
        "            train_params += [params]\n",
        "\n",
        "        for params in model.resNet.layer4[2].conv1.parameters():\n",
        "            params.requires_grad = True\n",
        "            train_params += [params]\n",
        "        #\n",
        "        for params in model.resNet.layer4[2].conv2.parameters():\n",
        "            params.requires_grad = True\n",
        "            train_params += [params]\n",
        "        #\n",
        "        for params in model.resNet.fc.parameters():\n",
        "            params.requires_grad = True\n",
        "            train_params += [params]\n",
        "\n",
        "        model.resNet.layer4[0].conv1.train(True)\n",
        "        model.resNet.layer4[0].conv2.train(True)\n",
        "        model.resNet.layer4[1].conv1.train(True)\n",
        "        model.resNet.layer4[1].conv2.train(True)\n",
        "        model.resNet.layer4[2].conv1.train(True)\n",
        "        model.resNet.layer4[2].conv2.train(True)\n",
        "        model.resNet.fc.train(True)\n",
        "\n",
        "    for params in model.lstm_cell.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "\n",
        "    for params in model.classifier.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "\n",
        "    return model, train_params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3paF7csiNRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_rgb(model, config, train_loader, val_loader):\n",
        "    wandb.watch(model, log=\"all\")\n",
        "    train_iter = 0\n",
        "    best_accuracy = 0\n",
        "    train = []\n",
        "    val = []\n",
        "    for epoch in range(config.epochs):\n",
        "        epoch_loss = 0\n",
        "        numCorrTrain = 0\n",
        "        trainSamples = 0\n",
        "        iterPerEpoch = 0\n",
        "        model.lstm_cell.train(True)\n",
        "        model.classifier.train(True)\n",
        "        # writer.add_scalar('lr', optimizer_fn.param_groups[0]['lr'], epoch+1)\n",
        "        if config.stage == 2:\n",
        "            model.resNet.layer4[0].conv1.train(True)\n",
        "            model.resNet.layer4[0].conv2.train(True)\n",
        "            model.resNet.layer4[1].conv1.train(True)\n",
        "            model.resNet.layer4[1].conv2.train(True)\n",
        "            model.resNet.layer4[2].conv1.train(True)\n",
        "            model.resNet.layer4[2].conv2.train(True)\n",
        "            model.resNet.fc.train(True)\n",
        "        for inputs, labels in train_loader:\n",
        "            train_iter += 1\n",
        "            iterPerEpoch += 1\n",
        "            optimizer_fn.zero_grad()\n",
        "            trainSamples += inputs.size(0)\n",
        "            inputs = inputs.permute(1, 0, 2, 3, 4).to(config.device)  # but why?\n",
        "            labels = labels.to(config.device)\n",
        "            output_label, _ = model(inputs)\n",
        "            loss = loss_fn(output_label, labels)\n",
        "            loss.backward()\n",
        "            optimizer_fn.step()\n",
        "            _, predicted = torch.max(output_label.data, 1)\n",
        "\n",
        "            predicted = predicted.to(config.device)\n",
        "            numCorrTrain += torch.sum(predicted == labels).data.item()\n",
        "\n",
        "            # numCorrTrain += (predicted == targets.cuda()).sum()\n",
        "            epoch_loss += loss.item()\n",
        "        optim_scheduler.step()\n",
        "        avg_loss = epoch_loss / iterPerEpoch\n",
        "        trainAccuracy = (numCorrTrain / trainSamples)\n",
        "\n",
        "        print('Train: Epoch = {}/{} | Loss = {} | Accuracy = {}'.format(epoch + 1, config.epochs, avg_loss, trainAccuracy))\n",
        "\n",
        "        max_loss = 6\n",
        "        avg_loss_normalized = avg_loss if avg_loss < max_loss else max_loss\n",
        "        train.append((trainAccuracy, avg_loss_normalized))\n",
        "        wandb.log({\"train_loss\": avg_loss_normalized,\n",
        "                   \"train_accuracy\": trainAccuracy,\n",
        "                   \"eopch\": (epoch + 1)})\n",
        "\n",
        "        if (epoch + 1) % config.val_frequency == 0:\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                val_loss_epoch = 0\n",
        "                val_iter = 0\n",
        "                val_samples = 0\n",
        "                numCorr = 0\n",
        "                for inputs, labels in val_loader:\n",
        "                    val_iter += 1\n",
        "                    val_samples += inputs.size(0)\n",
        "                    inputs = inputs.permute(1, 0, 2, 3, 4).to(config.device)\n",
        "                    labels = labels.to(config.device)\n",
        "                    output_label, _ = model(inputs)\n",
        "                    val_loss = loss_fn(output_label, labels)\n",
        "                    val_loss_epoch += val_loss.item()\n",
        "                    _, predicted = torch.max(output_label.data, 1)\n",
        "                    numCorr += torch.sum(predicted == labels).data.item()\n",
        "            val_accuracy = (numCorr / val_samples)\n",
        "            avg_val_loss = val_loss_epoch / val_iter\n",
        "            print('*****  Val: Epoch = {} | Loss {} | Accuracy = {} *****'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
        "\n",
        "            avg_val_loss_normalized = avg_val_loss if avg_val_loss < max_loss else max_loss\n",
        "            val.append((val_accuracy, avg_val_loss_normalized))\n",
        "            wandb.log({\"valid_loss\": avg_val_loss_normalized,\n",
        "                       \"valid_accuracy\": val_accuracy,\n",
        "                       \"eopch\": (epoch + 1)})\n",
        "\n",
        "            if val_accuracy > best_accuracy:\n",
        "                save_path_model = (config.models_dir + '/best_model_rgb_state_dict.pth')\n",
        "                torch.save(model.state_dict(), save_path_model)\n",
        "                best_accuracy = val_accuracy\n",
        "        else:\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                save_path_model = (config.models_dir + '/model_rgb_state_dict_epoch' + str(epoch + 1) + '.pth')\n",
        "                # torch.save(model.state_dict(), save_path_model)\n",
        "    wandb.run.summary[\"best_valid_accuracy\"] = best_accuracy\n",
        "    return train, val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3PWFNY_iTug",
        "colab_type": "text"
      },
      "source": [
        "Prepare Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiY0dGkmiS4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gtea_dataset import gtea61\n",
        "from spatial_transforms import *\n",
        "from objectAttentionModelConvLSTM import *\n",
        "\n",
        "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "spatial_transform = Compose([Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n",
        "                             ToTensor(), normalize])\n",
        "\n",
        "gtea_root = \"GTEA61\"\n",
        "config = config_stage1\n",
        "train_dataset = gtea61(\"rgb\", gtea_root, split=\"train\", user_split=config.training_user_split, seq_len_rgb=config.seq_len, transform_rgb=spatial_transform, preload=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "val_transform = Compose([Scale(256), CenterCrop(224), ToTensor(), normalize])\n",
        "val_dataset = gtea61(\"rgb\", gtea_root, split=\"test\", user_split=config.val_user_split, seq_len_rgb=config.seq_len, transform_rgb=val_transform, preload=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "print(f\"Train dataset: {len(train_dataset)} videos\")\n",
        "print(f\"Valid dataset: {len(val_dataset)} videos\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9-ANuGoiXki",
        "colab_type": "text"
      },
      "source": [
        "Train Stage 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6NmSOuKiW_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = config_stage1\n",
        "\n",
        "model, train_params = prepare_training_rgb(config)\n",
        "model.to(config.device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer_fn = torch.optim.Adam(train_params, lr=config.lr, weight_decay=config.weight_decay, eps=1e-4)\n",
        "optim_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=config.decay_steps, gamma=config.decay_factor)\n",
        "\n",
        "training_time = datetime.now().strftime(\"%d-%b_%H-%M\")\n",
        "wandb.init(config=config, group=f\"{config.seq_len}f\", name=f\"{training_time} Stage1, {config.seq_len}f, T{str(config.training_user_split).replace(' ', '')}\", project=\"mldl-fpar\")\n",
        "\n",
        "train_rgb, val_rgb = training_rgb(model, config, train_loader, val_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhlMePN2iaq7",
        "colab_type": "text"
      },
      "source": [
        "Train Stage 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVxgDka8iZLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = config_stage2\n",
        "\n",
        "model, train_params2 = prepare_training_rgb(config)\n",
        "model.lstm_cell.train(True)\n",
        "model.classifier.train(True)\n",
        "model.to(config.device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer_fn = torch.optim.Adam(train_params2, lr=config.lr, weight_decay=config.weight_decay, eps=1e-4)\n",
        "optim_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=config.decay_steps, gamma=config.decay_factor)\n",
        "\n",
        "\n",
        "training_time = datetime.now().strftime(\"%d-%b_%H-%M\")\n",
        "wandb.init(config=config, group=f\"{config.seq_len}f\", name=f\"{training_time} Stage2, {config.seq_len}f, T{str(config.training_user_split).replace(' ', '')}\", project=\"mldl-fpar\")\n",
        "\n",
        "train_rgb, val_rgb = training_rgb(model, config, train_loader, val_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpCHu-bliLMQ",
        "colab_type": "text"
      },
      "source": [
        "# Flow Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P1ehUIzipPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config_flow = Config({\"stage\": \"flow\",\n",
        "                      \"num_classes\": 61,\n",
        "                      \"batch_size\": 32,\n",
        "                      \"lstm_mem_size\": 512,\n",
        "                      \"lr\": 1e-2,\n",
        "                      \"optimizer\": \"sgd\",\n",
        "                      \"sgd_momentum\": 0.9,\n",
        "                      \"epochs\": 750,\n",
        "                      \"decay_steps\": [150, 300, 500],\n",
        "                      \"decay_factor\": 0.5,\n",
        "                      \"weight_decay\": 5e-4,\n",
        "                      \"val_frequency\": 3,\n",
        "                      \"models_dir\": \"models\",\n",
        "                      \"seq_len_flow\": 5,\n",
        "                      \"training_user_split\": [1, 3, 4],\n",
        "                      \"val_user_split\": [2]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxHfz5v1ise2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_flow(model, config, train_loader, val_loader):\n",
        "    wandb.watch(model, log=\"all\")\n",
        "    train_iter = 0\n",
        "    best_accuracy = 0\n",
        "    train = []\n",
        "    val = []\n",
        "    for epoch in range(config.epochs):\n",
        "        epoch_loss = 0\n",
        "        numCorrTrain = 0\n",
        "        trainSamples = 0\n",
        "        iterPerEpoch = 0\n",
        "        model.train(True)\n",
        "        for inputs, labels in train_loader:\n",
        "            train_iter += 1\n",
        "            iterPerEpoch += 1\n",
        "            optimizer_fn.zero_grad()\n",
        "            trainSamples += inputs.size(0)\n",
        "            inputs = inputs.to(config.device)\n",
        "            labels = labels.to(config.device)\n",
        "            output_label, _ = model(inputs)\n",
        "            loss = loss_fn(output_label, labels)\n",
        "            loss.backward()\n",
        "            optimizer_fn.step()\n",
        "            _, predicted = torch.max(output_label.data, 1)\n",
        "            numCorrTrain += torch.sum(predicted == labels).data.item()\n",
        "            epoch_loss += loss.item()\n",
        "        optim_scheduler.step()\n",
        "        avg_loss = epoch_loss / iterPerEpoch\n",
        "        trainAccuracy = (numCorrTrain / trainSamples)\n",
        "        print('Train: Epoch = {}/{} | Loss = {} | Accuracy = {}'.format(epoch + 1, config.epochs, avg_loss, trainAccuracy))\n",
        "\n",
        "        max_loss = 6\n",
        "        avg_loss_normalized = avg_loss if avg_loss < max_loss else max_loss\n",
        "        train.append((trainAccuracy, avg_loss_normalized))\n",
        "        wandb.log({\"train_loss\": avg_loss_normalized,\n",
        "                   \"train_accuracy\": trainAccuracy,\n",
        "                   \"eopch\": (epoch + 1)})\n",
        "\n",
        "        if (epoch + 1) % config.val_frequency == 0:\n",
        "            model.eval()\n",
        "            val_loss_epoch = 0\n",
        "            val_iter = 0\n",
        "            val_samples = 0\n",
        "            numCorr = 0\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    val_iter += 1\n",
        "                    val_samples += inputs.size(0)\n",
        "                    inputs = inputs.to(config.device)\n",
        "                    labels = labels.to(config.device)\n",
        "                    output_label, _ = model(inputs)\n",
        "                    val_loss = loss_fn(output_label, labels)\n",
        "                    val_loss_epoch += val_loss.item()\n",
        "                    _, predicted = torch.max(output_label.data, 1)\n",
        "                    numCorr += torch.sum(predicted == labels).data.item()\n",
        "            val_accuracy = (numCorr / val_samples)\n",
        "            avg_val_loss = val_loss_epoch / val_iter\n",
        "            print('*****  Validation: Epoch = {} | Loss = {} | Accuracy = {}  *****'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
        "            avg_val_loss_normalized = avg_val_loss if avg_val_loss < max_loss else max_loss\n",
        "            val.append((val_accuracy, avg_val_loss_normalized))\n",
        "            wandb.log({\"valid_loss\": avg_val_loss_normalized,\n",
        "                       \"valid_accuracy\": val_accuracy,\n",
        "                       \"eopch\": (epoch + 1)})\n",
        "            if val_accuracy > best_accuracy:\n",
        "                save_path_model = os.path.join(config.models_dir, \"best_model_flow_state_dict.pth\")\n",
        "                torch.save(model.state_dict(), save_path_model)\n",
        "                best_accuracy = val_accuracy\n",
        "        else:\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                save_path_model = os.path.join(config.models_dir, 'model_flow_state_dict_epoch' + str(epoch + 1) + '.pth')\n",
        "                # torch.save(model.state_dict(), save_path_model)\n",
        "    wandb.run.summary[\"best_valid_accuracy\"] = best_accuracy\n",
        "    return train, val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2ECBJPIiu3_",
        "colab_type": "text"
      },
      "source": [
        "Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTn5OIV-iuEw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "ad5c7693-d3f4-413d-ec78-ec6254eb90df"
      },
      "source": [
        "from gtea_dataset import gtea61\n",
        "from spatial_transforms import *\n",
        "from objectAttentionModelConvLSTM import *\n",
        "\n",
        "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "spatial_transform = Compose([Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n",
        "                             ToTensor(), normalize])\n",
        "\n",
        "gtea_root = \"GTEA61\"\n",
        "config = config_flow\n",
        "train_dataset = gtea61(\"flow\", gtea_root, split=\"train\", user_split=config.training_user_split, seq_len_flow=config.seq_len_flow, transform_flow=spatial_transform, preload=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, sampler=None, num_workers=4, pin_memory=True)\n",
        "\n",
        "val_transform = Compose([Scale(256), CenterCrop(224), ToTensor(), normalize])\n",
        "val_dataset = gtea61(\"flow\", gtea_root, split=\"test\", user_split=config.val_user_split, seq_len_flow=config.seq_len_flow, transform_flow=val_transform, preload=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train dataset: {len(train_dataset)} videos\")\n",
        "print(f\"Valid dataset: {len(val_dataset)} videos\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataset: 341 videos\n",
            "Valid dataset: 116 videos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB4A0OI1izd1",
        "colab_type": "text"
      },
      "source": [
        "Flow Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac3okP2siyyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flow_resnet import flow_resnet34\n",
        "\n",
        "model = flow_resnet34(True, channels=2 * config.seq_len_flow, num_classes=config.num_classes)\n",
        "model.train(True)\n",
        "train_params = list(model.parameters())\n",
        "model.to(config.device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer_fn = torch.optim.SGD(train_params, lr=config.lr, momentum=config.sgd_momentum, weight_decay=config.weight_decay)\n",
        "optim_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=config.decay_steps, gamma=config.decay_factor)\n",
        "\n",
        "training_time = datetime.now().strftime(\"%d-%b_%H-%M\")\n",
        "wandb.init(config=config, group=\"flow\", name=f\"{training_time} Flow, T{str(config.training_user_split).replace(' ', '')}\", project=\"mldl-fpar\")\n",
        "\n",
        "train_flow, val_flow = training_flow(model, config, train_loader, val_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IHUO4RSjcn5",
        "colab_type": "text"
      },
      "source": [
        "# TwoStream Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yn-t8qmjjWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spatial_transforms import (Compose, ToTensor, CenterCrop, Scale, Normalize, MultiScaleCornerCrop,\n",
        "                                RandomHorizontalFlip)\n",
        "from twoStreamModel import *\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FjsxYBSjsga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config_two_stream = Config({\"stage\": \"two_stream\",\n",
        "                            \"num_classes\": 61,\n",
        "                            \"batch_size\": 32,\n",
        "                            \"lstm_mem_size\": 512,\n",
        "                            \"lr\": 1e-2,\n",
        "                            \"optimizer\": \"adam\",\n",
        "                            \"epochs\": 250,\n",
        "                            \"decay_step\": 1,\n",
        "                            \"decay_factor\": 0.99,\n",
        "                            \"weight_decay\": 5e-4,\n",
        "                            \"val_frequency\": 3,\n",
        "                            \"sgd_momentum\": 0.9,\n",
        "                            \"models_dir\": \"models\",\n",
        "                            \"seq_len\": 7,\n",
        "                            \"seq_len_flow\": 5,\n",
        "                            \"training_user_split\": [1, 3, 4],\n",
        "                            \"val_user_split\": [2],\n",
        "                            \"two_stream_method\": \"average\"})  # two_stream_method is either average of joint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZQv1W3ejtj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_training_two_streams(config):\n",
        "    rgb_model_path = os.path.join(config.models_dir, \"best_model_rgb_state_dict.pth\")\n",
        "    flow_model_path = os.path.join(config.models_dir, \"best_model_flow_state_dict.pth\")\n",
        "    model = twoStreamAttentionModel(flow_model_path=flow_model_path, rgb_model_path=rgb_model_path, seq_len_flow=config.seq_len_flow, mem_size=config.lstm_mem_size, num_classes=config.num_classes, join_method=config.two_stream_method)\n",
        "\n",
        "    for params in model.parameters():\n",
        "        params.requires_grad = False\n",
        "\n",
        "    model.train(False)\n",
        "    train_params = []\n",
        "\n",
        "    for params in model.classifier.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "\n",
        "    for params in model.rgb_model.lstm_cell.parameters():\n",
        "        train_params += [params]\n",
        "        params.requires_grad = True\n",
        "\n",
        "    for params in model.rgb_model.resNet.layer4[0].conv1.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "\n",
        "    for params in model.rgb_model.resNet.layer4[0].conv2.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "\n",
        "    for params in model.rgb_model.resNet.layer4[1].conv1.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "\n",
        "    for params in model.rgb_model.resNet.layer4[1].conv2.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "\n",
        "    for params in model.rgb_model.resNet.layer4[2].conv1.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "    #\n",
        "    for params in model.rgb_model.resNet.layer4[2].conv2.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "    #\n",
        "    for params in model.rgb_model.resNet.fc.parameters():\n",
        "        params.requires_grad = True\n",
        "        train_params += [params]\n",
        "\n",
        "    base_params = []\n",
        "    for params in model.flow_model.layer4.parameters():\n",
        "        base_params += [params]\n",
        "        params.requires_grad = True\n",
        "\n",
        "    return model, train_params, base_params\n",
        "\n",
        "def training_two_streams(model, config, train_dataset, train_loader, val_dataset, val_loader):\n",
        "    train_iter = 0\n",
        "    train_samples = len(train_dataset)\n",
        "    val_samples = len(val_dataset)\n",
        "    best_accuracy = 0\n",
        "    for epoch in range(config.epochs):\n",
        "        epoch_loss = 0\n",
        "        numCorrTrain = 0\n",
        "        iterPerEpoch = 0\n",
        "        model.classifier.train(True)\n",
        "        model.flow_model.layer4.train(True)\n",
        "        for input_flow, input_rgb, labels in train_loader:\n",
        "            train_iter += 1\n",
        "            iterPerEpoch += 1\n",
        "            optimizer_fn.zero_grad()\n",
        "            input_flow = input_flow.to(config.device)\n",
        "            input_rgb = input_rgb.permute(1, 0, 2, 3, 4).to(config.device)\n",
        "            labels = labels.to(config.device)\n",
        "            output_label = model(input_flow, input_rgb)\n",
        "            loss = loss_fn(F.log_softmax(output_label, dim=1), labels)\n",
        "            # loss = loss_fn(output_label, labels)\n",
        "            loss.backward()\n",
        "            optimizer_fn.step()\n",
        "            _, predicted = torch.max(output_label.data, 1)\n",
        "            numCorrTrain += torch.sum(predicted == labels).data.item()\n",
        "            epoch_loss += loss.item()  # loss.data[0]\n",
        "        optim_scheduler.step()\n",
        "        avg_loss = epoch_loss / iterPerEpoch\n",
        "        trainAccuracy = (numCorrTrain / train_samples) * 100\n",
        "        print('Train: Epoch = {}/{} | Loss = {} | Accuracy = {}'.format(epoch + 1, config.epochs, avg_loss, trainAccuracy))\n",
        "        wandb.log({\"train_loss\": avg_loss,\n",
        "                   \"train_accuracy\": trainAccuracy,\n",
        "                   \"eopch\": (epoch + 1)})\n",
        "        if (epoch + 1) % config.val_frequency == 0:\n",
        "            model.eval()\n",
        "            val_loss_epoch = 0\n",
        "            val_iter = 0\n",
        "            numCorr = 0\n",
        "            for input_flow, input_rgb, labels in val_loader:\n",
        "                val_iter += 1\n",
        "                input_flow = input_flow.to(config.device)\n",
        "                input_rgb = input_rgb.permute(1, 0, 2, 3, 4).to(config.device)\n",
        "                labels = labels.to(config.device)\n",
        "                output_label = model(input_flow, input_rgb)\n",
        "                loss = loss_fn(F.log_softmax(output_label, dim=1), labels)\n",
        "                val_loss_epoch += loss.item()  # loss.data[0]\n",
        "                _, predicted = torch.max(output_label.data, 1)\n",
        "                numCorr += torch.sum(predicted == labels).data.item()\n",
        "            val_accuracy = (numCorr / val_samples) * 100\n",
        "            avg_val_loss = val_loss_epoch / val_iter\n",
        "            print('*****  Validation: Epoch = {} | Loss = {} | Accuracy = {}  *****'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
        "            wandb.log({\"valid_loss\": avg_val_loss,\n",
        "                       \"valid_accuracy\": val_accuracy,\n",
        "                       \"eopch\": (epoch + 1)})\n",
        "            if val_accuracy > best_accuracy:\n",
        "                save_path_model = os.path.join(config.models_dir, 'best_model_twoStream_state_dict.pth')\n",
        "                torch.save(model.state_dict(), save_path_model)\n",
        "                best_accuracy = val_accuracy\n",
        "        else:\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                save_path_model = os.path.join(config.models_dir, 'model_twoStream_state_dict_epoch' + str(epoch + 1) + '.pth')\n",
        "                # torch.save(model.state_dict(), save_path_model)\n",
        "    wandb.run.summary[\"best_valid_accuracy\"] = best_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gukyosrfj0B3",
        "colab_type": "text"
      },
      "source": [
        "Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QCeFot_jyTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gtea_dataset import gtea61\n",
        "from spatial_transforms import *\n",
        "from objectAttentionModelConvLSTM import *\n",
        "\n",
        "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "spatial_transform = Compose([Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n",
        "                                 ToTensor(), normalize])\n",
        "\n",
        "gtea_root = \"GTEA61\"\n",
        "config = config_two_stream\n",
        "train_dataset = gtea61(\"joint\", gtea_root, split=\"train\", user_split=config.training_user_split, seq_len_rgb=config.seq_len, seq_len_flow=config.seq_len_flow, transform_rgb=spatial_transform, transform_flow=spatial_transform, preload=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "val_transform = Compose([Scale(256), CenterCrop(224), ToTensor(), normalize])\n",
        "val_dataset = gtea61(\"joint\", gtea_root, split=\"test\", user_split=config.val_user_split, seq_len_rgb=config.seq_len, seq_len_flow=config.seq_len_flow, transform_rgb=val_transform, transform_flow=val_transform, preload=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4, pin_memory=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlolGUJ5j2RA",
        "colab_type": "text"
      },
      "source": [
        "2Stream Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2xmlPPBj4II",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = config_two_stream\n",
        "model, train_params, base_params = prepare_training_two_streams(config)\n",
        "model.to(config.device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer_fn = torch.optim.SGD([{'params': train_params}, {'params': base_params, 'lr': 1e-4}],\n",
        "                               lr=config.lr, momentum=config.sgd_momentum, weight_decay=config.weight_decay)\n",
        "optim_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_fn, step_size=config.decay_step, gamma=config.decay_factor)\n",
        "\n",
        "training_time = datetime.now().strftime(\"%d-%b_%H-%M\")\n",
        "wandb.init(config=config, group=f\"{config.seq_len}f\", name=f\"{training_time} 2Stream, {config.seq_len}f, T{str(config.training_user_split).replace(' ', '')}\", project=\"mldl-fpar\")\n",
        "\n",
        "training_two_streams(model, config, train_dataset, train_loader, val_dataset, val_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KI40zvDYYYf",
        "colab_type": "text"
      },
      "source": [
        "Generation Activation Maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvnOvOcMaa9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config_gen_map = Config({\"stage\": \"rgb\",\n",
        "                            \"num_classes\": 61,\n",
        "                            \"lstm_mem_size\": 512})  # two_stream_method is either average of joint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96rS8BzzYdp3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "7bef4a00-3771-4a23-bc0c-b5d4215c54bd"
      },
      "source": [
        "from objectAttentionModelConvLSTM import *\n",
        "from attentionMapModel import attentionMap\n",
        "import cv2\n",
        "\n",
        "# Path to the weights of the pre-trained model\n",
        "model_state_dict = 'Models/best_model_state_dict_rgb_split2.pth' \n",
        "\n",
        "model = attentionModel(num_classes=config_gen_map.num_classes, mem_size=config_gen_map.lstm_mem_size)\n",
        "model.load_state_dict(torch.load(model_state_dict))\n",
        "model_backbone = model.resNet\n",
        "attentionMapModel = attentionMap(model_backbone).to(config.device)\n",
        "attentionMapModel.train(False)\n",
        "for params in attentionMapModel.parameters():\n",
        "    params.requires_grad = False\n",
        "\n",
        "normalize = transforms.Normalize(\n",
        "   mean=[0.485, 0.456, 0.406],\n",
        "   std=[0.229, 0.224, 0.225]\n",
        ")\n",
        "preprocess1 = transforms.Compose([ transforms.Scale(256), transforms.CenterCrop(224) ])\n",
        "\n",
        "preprocess2 = transforms.Compose([ transforms.ToTensor(), normalize])\n",
        "\n",
        "path_image = 'S1/close_chocolate/1/rgb/rgb0001.png'\n",
        "fl_name_in = 'GTEA61/processed_frames2/' + path_image\n",
        "\n",
        "output_folder = 'Images'\n",
        "if not os.path.isdir(output_folder):\n",
        "    os.mkdir(output_folder)\n",
        "fl_name_out = output_folder + '/' + path_image[-11:]\n",
        "\n",
        "img_pil = Image.open(fl_name_in)\n",
        "img_pil1 = preprocess1(img_pil)\n",
        "img_size = img_pil1.size\n",
        "size_upsample = (img_size[0], img_size[1])\n",
        "img_tensor = preprocess2(img_pil1)\n",
        "img_variable = Variable(img_tensor.unsqueeze(0).to(config.device))\n",
        "img = np.asarray(img_pil1)\n",
        "attentionMap_image = attentionMapModel(img_variable, img, size_upsample)\n",
        "cv2.imwrite(fl_name_out, attentionMap_image)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:220: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMLYSMeRfiU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}